{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a420923",
   "metadata": {},
   "source": [
    "# Limpieza y PreparaciÃ³n de Datos | Proyecto Aurelion\n",
    "\n",
    "---\n",
    "\n",
    "## InformaciÃ³n del Proyecto\n",
    "\n",
    "| **Campo** | **Detalle** |\n",
    "|-----------|-------------|\n",
    "| **Proyecto** | Minimarket Aurelion - Pipeline de Datos Comerciales |\n",
    "| **Sprint** | Sprint 2 - AnÃ¡lisis Exploratorio de Datos |\n",
    "| **Notebook** | `7. Limpieza_datos.ipynb` |\n",
    "| **Autor** | Diego Armando VÃ¡squez ChÃ¡vez |\n",
    "| **Email** | [emailÂ protected] |\n",
    "| **Fecha** | 3 de noviembre de 2025 |\n",
    "| **Mentor** | Mirta Gladys Julio |\n",
    "\n",
    "---\n",
    "\n",
    "## PropÃ³sito del Notebook\n",
    "\n",
    "Este notebook ejecuta el proceso profesional de limpieza y preparaciÃ³n de datos del Minimarket Aurelion, aplicando tÃ©cnicas avanzadas de calidad de datos, validaciÃ³n de integridad referencial y normalizaciÃ³n. Constituye la fase crÃ­tica del pipeline que garantiza datasets confiables para anÃ¡lisis posteriores y modelado de Machine Learning.\n",
    "\n",
    "## Objetivos Principales\n",
    "\n",
    "### InspecciÃ³n y DiagnÃ³stico\n",
    "- AnÃ¡lisis exhaustivo de calidad de datos en datasets originales\n",
    "- IdentificaciÃ³n de valores nulos, duplicados y inconsistencias\n",
    "- EvaluaciÃ³n de integridad referencial entre tablas relacionales\n",
    "\n",
    "### Tratamiento de Problemas\n",
    "- CorrecciÃ³n de tipos de datos y formatos inconsistentes\n",
    "- EliminaciÃ³n inteligente de duplicados preservando informaciÃ³n crÃ­tica\n",
    "- ImputaciÃ³n estratÃ©gica de valores faltantes segÃºn reglas de negocio\n",
    "\n",
    "### ValidaciÃ³n y NormalizaciÃ³n\n",
    "- AplicaciÃ³n de reglas de validaciÃ³n de integridad referencial\n",
    "- NormalizaciÃ³n de campos categÃ³ricos y estandarizaciÃ³n de formatos\n",
    "- VerificaciÃ³n de rangos vÃ¡lidos y coherencia lÃ³gica\n",
    "\n",
    "### DocumentaciÃ³n y Trazabilidad\n",
    "- Registro detallado de transformaciones aplicadas\n",
    "- MÃ©tricas de calidad antes/despuÃ©s del proceso\n",
    "- ExportaciÃ³n de datasets limpios con logs de auditorÃ­a\n",
    "\n",
    "---\n",
    "\n",
    "## Etapas Principales de EjecuciÃ³n\n",
    "\n",
    "1ï¸âƒ£ **Setup y ConfiguraciÃ³n Inicial**  \n",
    "ConfiguraciÃ³n del entorno de limpieza con logging estructurado, definiciÃ³n de rutas y carga de parÃ¡metros.\n",
    "\n",
    "2ï¸âƒ£ **InspecciÃ³n Inicial Exhaustiva**  \n",
    "AnÃ¡lisis de estructura, tipos y dimensiones de cada dataset, identificando campos crÃ­ticos y problemas iniciales.\n",
    "\n",
    "3ï¸âƒ£ **IdentificaciÃ³n y CatalogaciÃ³n de Problemas**  \n",
    "DetecciÃ³n de nulos, duplicados, tipos incorrectos e inconsistencias referenciales.\n",
    "\n",
    "4ï¸âƒ£ **Tratamiento EstratÃ©gico de Nulos**  \n",
    "Imputaciones y descartes siguiendo prioridades de negocio.\n",
    "\n",
    "5ï¸âƒ£ **EliminaciÃ³n Inteligente de Duplicados**  \n",
    "DepuraciÃ³n por claves y filas completas, preservando registros de mayor valor.\n",
    "\n",
    "6ï¸âƒ£ **CorrecciÃ³n de Tipos y Formatos**  \n",
    "ConversiÃ³n a tipos adecuados, normalizaciÃ³n de fechas y escalas numÃ©ricas.\n",
    "\n",
    "7ï¸âƒ£ **NormalizaciÃ³n y EstandarizaciÃ³n**  \n",
    "Title Case, validaciÃ³n de rangos y cÃ¡lculo de mÃ©tricas derivadas.\n",
    "\n",
    "8ï¸âƒ£ **Validaciones Finales y Control de Calidad**  \n",
    "Pruebas de integridad post-limpieza y mÃ©tricas comparativas.\n",
    "\n",
    "9ï¸âƒ£ **Conclusiones y ExportaciÃ³n**  \n",
    "Resumen ejecutivo, exportaciÃ³n y trazabilidad.\n",
    "\n",
    "---\n",
    "\n",
    "## Datasets de Entrada y Salida\n",
    "\n",
    "**Entrada (Excel)**\n",
    "- `clientes.xlsx`: InformaciÃ³n de clientes del minimarket\n",
    "- `productos.xlsx`: CatÃ¡logo de productos con precios base\n",
    "- `ventas.xlsx`: Transacciones principales con fechas y medios de pago\n",
    "- `detalle_ventas.xlsx`: LÃ­neas de detalle con cantidades y precios\n",
    "\n",
    "**Salida (CSV limpios)**\n",
    "- `datasets_limpios/clientes_clean.csv`: Clientes validados y normalizados\n",
    "- `datasets_limpios/productos_clean.csv`: Productos con datos consistentes\n",
    "- `datasets_limpios/ventas_clean.csv`: Ventas con integridad verificada\n",
    "- `datasets_limpios/detalle_clean.csv`: Detalle con validaciÃ³n referencial\n",
    "\n",
    "**Reportes de calidad**\n",
    "- `export/resumen_limpieza.csv`: MÃ©tricas comparativas del proceso\n",
    "- Logs de trazabilidad con transformaciones aplicadas\n",
    "\n",
    "---\n",
    "\n",
    "## Resultados Esperados\n",
    "\n",
    "âœ… Datasets de alta calidad con integridad referencial garantizada  \n",
    "âœ… EliminaciÃ³n completa de duplicados y valores inconsistentes  \n",
    "âœ… NormalizaciÃ³n de formatos y tipos de datos  \n",
    "âœ… Trazabilidad completa de transformaciones aplicadas  \n",
    "âœ… Base sÃ³lida para anÃ¡lisis exploratorio y modelado posterior\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Ãndice de Contenidos\n",
    "\n",
    "| **SecciÃ³n** | **DescripciÃ³n** |\n",
    "|-------------|-----------------|\n",
    "| 1. Setup y ConfiguraciÃ³n | InicializaciÃ³n y carga de configuraciones |\n",
    "| 2. InspecciÃ³n Inicial | AnÃ¡lisis de dimensiones y calidad inicial |\n",
    "| 3. IdentificaciÃ³n de Problemas | CatalogaciÃ³n de issues de calidad |\n",
    "| 4. Tratamiento de Nulos | Estrategias de imputaciÃ³n y eliminaciÃ³n |\n",
    "| 5. EliminaciÃ³n de Duplicados | DeduplicaciÃ³n inteligente |\n",
    "| 6. CorrecciÃ³n de Tipos | ConversiÃ³n a tipos correctos |\n",
    "| 7. NormalizaciÃ³n y EstandarizaciÃ³n | EstandarizaciÃ³n de formatos |\n",
    "| 8. Validaciones Finales | Controles de calidad |\n",
    "| 9. Conclusiones y ExportaciÃ³n | Resumen y exportaciÃ³n |\n",
    "| Reglas adicionales | Reglas de negocio complementarias |\n",
    "\n",
    "*ðŸ”§ Fase: PreparaciÃ³n de Datos | ðŸŽ¯ Objetivo: Datasets de Calidad para EDA*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d432765c",
   "metadata": {},
   "source": [
    "## 1. Setup y ConfiguraciÃ³n\n",
    "Importamos bibliotecas, definimos rutas y funciones auxiliares. Cargamos configuraciÃ³n centralizada desde `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4962970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup completo\n",
      "   Proyecto: c:\\Users\\LENOVO\\Desktop\\FUNDAMENTOS DE IA GUAYERD\\Diego VÃ¡squez - Proyecto Aurelion\n",
      "   Datasets: c:\\Users\\LENOVO\\Desktop\\FUNDAMENTOS DE IA GUAYERD\\Diego VÃ¡squez - Proyecto Aurelion\\datasets\n",
      "   Salida: c:\\Users\\LENOVO\\Desktop\\FUNDAMENTOS DE IA GUAYERD\\Diego VÃ¡squez - Proyecto Aurelion\\datasets_limpios\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Rutas\n",
    "DIR_PROYECTO = Path.cwd()\n",
    "DIR_DATASETS = DIR_PROYECTO / 'datasets'\n",
    "DIR_DATASETS_LIMPIOS = DIR_PROYECTO / 'datasets_limpios'\n",
    "DIR_EXPORT = DIR_PROYECTO / 'export'\n",
    "DIR_LOGS = DIR_PROYECTO / 'logs'\n",
    "\n",
    "# Crear directorios si no existen\n",
    "DIR_DATASETS_LIMPIOS.mkdir(exist_ok=True)\n",
    "DIR_EXPORT.mkdir(exist_ok=True)\n",
    "DIR_LOGS.mkdir(exist_ok=True)\n",
    "\n",
    "# Cargar config\n",
    "ruta_config = DIR_PROYECTO / 'config.json'\n",
    "if ruta_config.exists():\n",
    "    with open(ruta_config, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "else:\n",
    "    config = {}\n",
    "\n",
    "print(\"âœ… Setup completo\")\n",
    "print(f\"   Proyecto: {DIR_PROYECTO}\")\n",
    "print(f\"   Datasets: {DIR_DATASETS}\")\n",
    "print(f\"   Salida: {DIR_DATASETS_LIMPIOS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b7fcf",
   "metadata": {},
   "source": [
    "## 2. InspecciÃ³n Inicial\n",
    "Cargamos los datasets originales desde Excel y realizamos una inspecciÃ³n bÃ¡sica de dimensiones, tipos y valores nulos.\n",
    "\n",
    "â¬†ï¸ Volver al Ã­ndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42999cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š InspecciÃ³n inicial:\n",
      "\n",
      "clientes:\n",
      "   Dimensiones: 1,296 filas x 5 columnas\n",
      "   Nulos: 30 valores faltantes\n",
      "   Duplicados: 94 filas duplicadas completas\n",
      "\n",
      "productos:\n",
      "   Dimensiones: 151 filas x 5 columnas\n",
      "   Nulos: 3 valores faltantes\n",
      "   Duplicados: 11 filas duplicadas completas\n",
      "\n",
      "ventas:\n",
      "   Dimensiones: 12,960 filas x 9 columnas\n",
      "   Nulos: 27796 valores faltantes\n",
      "   Duplicados: 36 filas duplicadas completas\n",
      "\n",
      "detalle_ventas:\n",
      "   Dimensiones: 12,960 filas x 5 columnas\n",
      "   Nulos: 311 valores faltantes\n",
      "   Duplicados: 922 filas duplicadas completas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar datasets originales\n",
    "clientes = pd.read_excel(DIR_DATASETS / 'clientes.xlsx')\n",
    "productos = pd.read_excel(DIR_DATASETS / 'productos.xlsx')\n",
    "ventas = pd.read_excel(DIR_DATASETS / 'ventas.xlsx')\n",
    "detalle = pd.read_excel(DIR_DATASETS / 'detalle_ventas.xlsx')\n",
    "\n",
    "# Guardar copias originales\n",
    "orig_clientes = clientes.copy()\n",
    "orig_productos = productos.copy()\n",
    "orig_ventas = ventas.copy()\n",
    "orig_detalle = detalle.copy()\n",
    "\n",
    "# InspecciÃ³n bÃ¡sica\n",
    "datasets = {\n",
    "    'clientes': clientes,\n",
    "    'productos': productos,\n",
    "    'ventas': ventas,\n",
    "    'detalle_ventas': detalle\n",
    "}\n",
    "\n",
    "print(\"ðŸ“Š InspecciÃ³n inicial:\\n\")\n",
    "for nombre, df in datasets.items():\n",
    "    print(f\"{nombre}:\")\n",
    "    print(f\"   Dimensiones: {df.shape[0]:,} filas x {df.shape[1]} columnas\")\n",
    "    print(f\"   Nulos: {df.isnull().sum().sum()} valores faltantes\")\n",
    "    print(f\"   Duplicados: {df.duplicated().sum()} filas duplicadas completas\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01c255",
   "metadata": {},
   "source": [
    "## 3. IdentificaciÃ³n de Problemas\n",
    "Identificamos problemas de calidad especÃ­ficos: columnas con alta tasa de nulos, tipos incorrectos, valores fuera de rango, duplicados en claves primarias e inconsistencias de integridad referencial.\n",
    "\n",
    "â¬†ï¸ Volver al Ã­ndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba9b29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Problemas identificados:\n",
      "\n",
      "clientes - Columnas con nulos:\n",
      "   nombre_cliente: 0.54%\n",
      "   email: 1.16%\n",
      "   ciudad: 0.46%\n",
      "   fecha_alta: 0.15%\n",
      "productos - Columnas con nulos:\n",
      "   nombre_producto: 0.66%\n",
      "   categoria: 0.66%\n",
      "   precio_unitario: 0.66%\n",
      "ventas - Columnas con nulos:\n",
      "   fecha: 0.49%\n",
      "   id_cliente: 0.52%\n",
      "   medio_pago: 1.16%\n",
      "   canal: 0.22%\n",
      "   csat_1a5: 19.41%\n",
      "   ces_1a5: 25.82%\n",
      "   nps_0a10: 83.43%\n",
      "   nps_segmento: 83.43%\n",
      "detalle_ventas - Columnas con nulos:\n",
      "   id_producto: 0.25%\n",
      "   cantidad: 1.03%\n",
      "   precio_unitario: 0.66%\n",
      "   importe: 0.46%\n",
      "\n",
      "ðŸ”‘ Duplicados en claves primarias:\n",
      "   clientes.id_cliente: 96 duplicados\n",
      "   productos.id_producto: 11 duplicados\n",
      "   ventas.id_venta: 960 duplicados\n",
      "\n",
      "ðŸ”— Integridad referencial:\n",
      "   ventas con id_cliente inexistente: 68\n",
      "   detalle con id_producto inexistente: 32\n",
      "\n",
      "âœ… IdentificaciÃ³n de problemas completa\n"
     ]
    }
   ],
   "source": [
    "problemas = {}\n",
    "\n",
    "# 3.1 AnÃ¡lisis de nulos por columna\n",
    "print(\"âš ï¸ Problemas identificados:\\n\")\n",
    "for nombre, df in datasets.items():\n",
    "    nulos_col = df.isnull().sum()\n",
    "    pct_nulos = (nulos_col / len(df) * 100).round(2)\n",
    "    cols_con_nulos = pct_nulos[pct_nulos > 0].to_dict()\n",
    "    if cols_con_nulos:\n",
    "        problemas[f\"{nombre}_nulos\"] = cols_con_nulos\n",
    "        print(f\"{nombre} - Columnas con nulos:\")\n",
    "        for col, pct in cols_con_nulos.items():\n",
    "            print(f\"   {col}: {pct}%\")\n",
    "\n",
    "# 3.2 Verificar claves primarias duplicadas\n",
    "print(\"\\nðŸ”‘ Duplicados en claves primarias:\")\n",
    "if 'id_cliente' in clientes.columns and clientes['id_cliente'].duplicated().any():\n",
    "    print(f\"   clientes.id_cliente: {clientes['id_cliente'].duplicated().sum()} duplicados\")\n",
    "if 'id_producto' in productos.columns and productos['id_producto'].duplicated().any():\n",
    "    print(f\"   productos.id_producto: {productos['id_producto'].duplicated().sum()} duplicados\")\n",
    "if 'id_venta' in ventas.columns and ventas['id_venta'].duplicated().any():\n",
    "    print(f\"   ventas.id_venta: {ventas['id_venta'].duplicated().sum()} duplicados\")\n",
    "\n",
    "# 3.3 Integridad referencial\n",
    "print(\"\\nðŸ”— Integridad referencial:\")\n",
    "id_clientes_validos = set(clientes['id_cliente']) if 'id_cliente' in clientes.columns else set()\n",
    "id_productos_validos = set(productos['id_producto']) if 'id_producto' in productos.columns else set()\n",
    "id_ventas_validos = set(ventas['id_venta']) if 'id_venta' in ventas.columns else set()\n",
    "\n",
    "huerfanos_vc = ventas[~ventas['id_cliente'].isin(id_clientes_validos)] if 'id_cliente' in ventas.columns else pd.DataFrame()\n",
    "huerfanos_dp = detalle[~detalle['id_producto'].isin(id_productos_validos)] if 'id_producto' in detalle.columns else pd.DataFrame()\n",
    "huerfanos_dv = detalle[~detalle['id_venta'].isin(id_ventas_validos)] if 'id_venta' in detalle.columns else pd.DataFrame()\n",
    "\n",
    "if len(huerfanos_vc) > 0:\n",
    "    print(f\"   ventas con id_cliente inexistente: {len(huerfanos_vc)}\")\n",
    "if len(huerfanos_dp) > 0:\n",
    "    print(f\"   detalle con id_producto inexistente: {len(huerfanos_dp)}\")\n",
    "if len(huerfanos_dv) > 0:\n",
    "    print(f\"   detalle con id_venta inexistente: {len(huerfanos_dv)}\")\n",
    "\n",
    "print(\"\\nâœ… IdentificaciÃ³n de problemas completa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cca925",
   "metadata": {},
   "source": [
    "## 4. Tratamiento de Nulos\n",
    "Eliminamos o imputamos valores nulos segÃºn reglas de negocio. Para campos crÃ­ticos (IDs, precios, fechas) eliminamos filas; para campos opcionales conservamos la fila.\n",
    "\n",
    "â¬†ï¸ Volver al Ã­ndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85af4804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Tratamiento de nulos:\n",
      "\n",
      "clientes: 30 â†’ 15 nulos | 15 filas eliminadas\n",
      "productos: 3 â†’ 2 nulos | 1 filas eliminadas\n",
      "ventas: 27796 â†’ 26897 nulos | 278 filas eliminadas\n",
      "detalle_ventas: 311 â†’ 58 nulos | 251 filas eliminadas\n",
      "\n",
      "âœ… Tratamiento de nulos completo\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ§¹ Tratamiento de nulos:\\n\")\n",
    "\n",
    "# Campos crÃ­ticos (eliminar si estÃ¡n nulos)\n",
    "campos_criticos_clientes = ['id_cliente', 'nombre', 'email']\n",
    "campos_criticos_productos = ['id_producto', 'nombre', 'precio_base', 'categoria']\n",
    "campos_criticos_ventas = ['id_venta', 'id_cliente', 'fecha', 'medio_pago']\n",
    "campos_criticos_detalle = ['id_venta', 'id_producto', 'cantidad', 'precio_unitario']\n",
    "\n",
    "# Clientes: eliminar nulos en campos crÃ­ticos\n",
    "nulos_antes = clientes.isnull().sum().sum()\n",
    "for col in campos_criticos_clientes:\n",
    "    if col in clientes.columns:\n",
    "        clientes = clientes.dropna(subset=[col])\n",
    "nulos_despues = clientes.isnull().sum().sum()\n",
    "print(f\"clientes: {nulos_antes} â†’ {nulos_despues} nulos | {len(orig_clientes) - len(clientes)} filas eliminadas\")\n",
    "\n",
    "# Productos\n",
    "nulos_antes = productos.isnull().sum().sum()\n",
    "for col in campos_criticos_productos:\n",
    "    if col in productos.columns:\n",
    "        productos = productos.dropna(subset=[col])\n",
    "nulos_despues = productos.isnull().sum().sum()\n",
    "print(f\"productos: {nulos_antes} â†’ {nulos_despues} nulos | {len(orig_productos) - len(productos)} filas eliminadas\")\n",
    "\n",
    "# Ventas\n",
    "nulos_antes = ventas.isnull().sum().sum()\n",
    "for col in campos_criticos_ventas:\n",
    "    if col in ventas.columns:\n",
    "        ventas = ventas.dropna(subset=[col])\n",
    "nulos_despues = ventas.isnull().sum().sum()\n",
    "print(f\"ventas: {nulos_antes} â†’ {nulos_despues} nulos | {len(orig_ventas) - len(ventas)} filas eliminadas\")\n",
    "\n",
    "# Detalle\n",
    "nulos_antes = detalle.isnull().sum().sum()\n",
    "for col in campos_criticos_detalle:\n",
    "    if col in detalle.columns:\n",
    "        detalle = detalle.dropna(subset=[col])\n",
    "nulos_despues = detalle.isnull().sum().sum()\n",
    "print(f\"detalle_ventas: {nulos_antes} â†’ {nulos_despues} nulos | {len(orig_detalle) - len(detalle)} filas eliminadas\")\n",
    "\n",
    "print(\"\\nâœ… Tratamiento de nulos completo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b03db",
   "metadata": {},
   "source": [
    "## 5. EliminaciÃ³n de Duplicados\n",
    "Eliminamos filas duplicadas completas y duplicados en claves primarias (conservando la primera ocurrencia).\n",
    "\n",
    "â¬†ï¸ Volver al Ã­ndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e4c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—‘ï¸ EliminaciÃ³n de duplicados:\n",
      "\n",
      "clientes.id_cliente: 94 â†’ 0 duplicados\n",
      "productos.id_producto: 11 â†’ 0 duplicados\n",
      "ventas.id_venta: 920 â†’ 0 duplicados\n",
      "detalle_ventas (filas completas): 922 â†’ 0 duplicados\n",
      "\n",
      "âœ… EliminaciÃ³n de duplicados completa\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ—‘ï¸ EliminaciÃ³n de duplicados:\\n\")\n",
    "\n",
    "# Clientes: duplicados en id_cliente\n",
    "dup_antes = clientes.duplicated(subset=['id_cliente']).sum() if 'id_cliente' in clientes.columns else 0\n",
    "clientes = clientes.drop_duplicates(subset=['id_cliente'], keep='first') if 'id_cliente' in clientes.columns else clientes\n",
    "dup_despues = clientes.duplicated(subset=['id_cliente']).sum() if 'id_cliente' in clientes.columns else 0\n",
    "print(f\"clientes.id_cliente: {dup_antes} â†’ {dup_despues} duplicados\")\n",
    "\n",
    "# Productos: duplicados en id_producto\n",
    "dup_antes = productos.duplicated(subset=['id_producto']).sum() if 'id_producto' in productos.columns else 0\n",
    "productos = productos.drop_duplicates(subset=['id_producto'], keep='first') if 'id_producto' in productos.columns else productos\n",
    "dup_despues = productos.duplicated(subset=['id_producto']).sum() if 'id_producto' in productos.columns else 0\n",
    "print(f\"productos.id_producto: {dup_antes} â†’ {dup_despues} duplicados\")\n",
    "\n",
    "# Ventas: duplicados en id_venta\n",
    "dup_antes = ventas.duplicated(subset=['id_venta']).sum() if 'id_venta' in ventas.columns else 0\n",
    "ventas = ventas.drop_duplicates(subset=['id_venta'], keep='first') if 'id_venta' in ventas.columns else ventas\n",
    "dup_despues = ventas.duplicated(subset=['id_venta']).sum() if 'id_venta' in ventas.columns else 0\n",
    "print(f\"ventas.id_venta: {dup_antes} â†’ {dup_despues} duplicados\")\n",
    "\n",
    "# Detalle: duplicados completos\n",
    "dup_antes = detalle.duplicated().sum()\n",
    "detalle = detalle.drop_duplicates(keep='first')\n",
    "dup_despues = detalle.duplicated().sum()\n",
    "print(f\"detalle_ventas (filas completas): {dup_antes} â†’ {dup_despues} duplicados\")\n",
    "\n",
    "print(\"\\nâœ… EliminaciÃ³n de duplicados completa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615df63",
   "metadata": {},
   "source": [
    "## 6. CorrecciÃ³n de Tipos\n",
    "Convertimos columnas a sus tipos correctos: fechas a `datetime`, IDs a `int`, precios/cantidades a `float`, categorÃ­as a `str`.\n",
    "\n",
    "â¬†ï¸ Volver al Ã­ndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c367e424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ CorrecciÃ³n de tipos:\n",
      "\n",
      "clientes: id_cliente â†’ int, fecha_alta â†’ datetime\n",
      "productos: id_producto â†’ int, precio_base â†’ float\n",
      "ventas: id_venta, id_cliente â†’ int, fecha â†’ datetime\n",
      "detalle_ventas: IDs â†’ int, cantidad/precio â†’ float\n",
      "\n",
      "âœ… CorrecciÃ³n de tipos completa\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”§ CorrecciÃ³n de tipos:\\n\")\n",
    "\n",
    "# Clientes\n",
    "if 'id_cliente' in clientes.columns:\n",
    "    clientes['id_cliente'] = clientes['id_cliente'].astype(int)\n",
    "if 'fecha_alta' in clientes.columns:\n",
    "    clientes['fecha_alta'] = pd.to_datetime(clientes['fecha_alta'], errors='coerce')\n",
    "for col in ['nombre', 'email']:\n",
    "    if col in clientes.columns:\n",
    "        clientes[col] = clientes[col].astype(str)\n",
    "print(\"clientes: id_cliente â†’ int, fecha_alta â†’ datetime\")\n",
    "\n",
    "# Productos\n",
    "if 'id_producto' in productos.columns:\n",
    "    productos['id_producto'] = productos['id_producto'].astype(int)\n",
    "if 'precio_base' in productos.columns:\n",
    "    productos['precio_base'] = pd.to_numeric(productos['precio_base'], errors='coerce')\n",
    "for col in ['nombre', 'categoria']:\n",
    "    if col in productos.columns:\n",
    "        productos[col] = productos[col].astype(str)\n",
    "print(\"productos: id_producto â†’ int, precio_base â†’ float\")\n",
    "\n",
    "# Ventas\n",
    "if 'id_venta' in ventas.columns:\n",
    "    ventas['id_venta'] = ventas['id_venta'].astype(int)\n",
    "if 'id_cliente' in ventas.columns:\n",
    "    ventas['id_cliente'] = ventas['id_cliente'].astype(int)\n",
    "if 'fecha' in ventas.columns:\n",
    "    ventas['fecha'] = pd.to_datetime(ventas['fecha'], errors='coerce')\n",
    "if 'medio_pago' in ventas.columns:\n",
    "    ventas['medio_pago'] = ventas['medio_pago'].astype(str)\n",
    "print(\"ventas: id_venta, id_cliente â†’ int, fecha â†’ datetime\")\n",
    "\n",
    "# Detalle\n",
    "for col in ['id_venta', 'id_producto']:\n",
    "    if col in detalle.columns:\n",
    "        detalle[col] = detalle[col].astype(int)\n",
    "for col in ['cantidad', 'precio_unitario']:\n",
    "    if col in detalle.columns:\n",
    "        detalle[col] = pd.to_numeric(detalle[col], errors='coerce')\n",
    "print(\"detalle_ventas: IDs â†’ int, cantidad/precio â†’ float\")\n",
    "\n",
    "print(\"\\nâœ… CorrecciÃ³n de tipos completa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7174ae6",
   "metadata": {},
   "source": [
    "## 7. NormalizaciÃ³n y EstandarizaciÃ³n\n",
    "Normalizamos nombres de columnas, estandarizamos valores categÃ³ricos y validamos rangos de valores numÃ©ricos.\n",
    "\n",
    "â¬†ï¸ Volver al Ã­ndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0257e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ NormalizaciÃ³n y estandarizaciÃ³n:\n",
      "\n",
      "âœ“ Nombres de columnas normalizados (minÃºsculas)\n",
      "âœ“ CategorÃ­as estandarizadas (Title Case)\n",
      "âœ“ Rangos numÃ©ricos validados (valores positivos)\n",
      "âœ“ Campo 'importe_total' calculado\n",
      "\n",
      "âœ… NormalizaciÃ³n completa\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“ NormalizaciÃ³n y estandarizaciÃ³n:\\n\")\n",
    "\n",
    "# Normalizar nombres de columnas a minÃºsculas\n",
    "clientes.columns = clientes.columns.str.lower().str.strip()\n",
    "productos.columns = productos.columns.str.lower().str.strip()\n",
    "ventas.columns = ventas.columns.str.lower().str.strip()\n",
    "detalle.columns = detalle.columns.str.lower().str.strip()\n",
    "print(\"âœ“ Nombres de columnas normalizados (minÃºsculas)\")\n",
    "\n",
    "# Estandarizar categorÃ­as\n",
    "if 'categoria' in productos.columns:\n",
    "    productos['categoria'] = productos['categoria'].astype(str).str.strip().str.title()\n",
    "if 'medio_pago' in ventas.columns:\n",
    "    ventas['medio_pago'] = ventas['medio_pago'].astype(str).str.strip().str.title()\n",
    "print(\"âœ“ CategorÃ­as estandarizadas (Title Case)\")\n",
    "\n",
    "# Validar rangos numÃ©ricos\n",
    "if 'precio_base' in productos.columns:\n",
    "    productos = productos[productos['precio_base'] > 0]\n",
    "if 'cantidad' in detalle.columns:\n",
    "    detalle = detalle[detalle['cantidad'] > 0]\n",
    "if 'precio_unitario' in detalle.columns:\n",
    "    detalle = detalle[detalle['precio_unitario'] > 0]\n",
    "print(\"âœ“ Rangos numÃ©ricos validados (valores positivos)\")\n",
    "\n",
    "# Calcular importe_total si no existe\n",
    "if 'importe_total' not in detalle.columns and {'cantidad', 'precio_unitario'}.issubset(detalle.columns):\n",
    "    detalle['importe_total'] = detalle['cantidad'] * detalle['precio_unitario']\n",
    "    print(\"âœ“ Campo 'importe_total' calculado\")\n",
    "\n",
    "print(\"\\nâœ… NormalizaciÃ³n completa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea73659",
   "metadata": {},
   "source": [
    "## 8. Validaciones Finales\n",
    "Realizamos validaciones finales de integridad referencial y coherencia de datos. Verificamos que no queden huÃ©rfanos y que las dimensiones sean consistentes.\n",
    "\n",
    "â¬†ï¸ Volver al Ã­ndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aad236d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Validaciones finales:\n",
      "\n",
      "âœ“ Ventas con id_cliente vÃ¡lido: 11728\n",
      "âœ“ Detalle con id_producto vÃ¡lido: 11533\n",
      "âœ“ Detalle con id_venta vÃ¡lido: 11533\n",
      "âœ“ Sin nulos en claves primarias\n",
      "âœ“ Sin duplicados en claves primarias\n",
      "\n",
      "âœ… Validaciones finales pasadas\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ” Validaciones finales:\\n\")\n",
    "\n",
    "# Reconstruir conjuntos actualizados de IDs vÃ¡lidos\n",
    "id_clientes_validos = set(clientes['id_cliente']) if 'id_cliente' in clientes.columns else set()\n",
    "id_productos_validos = set(productos['id_producto']) if 'id_producto' in productos.columns else set()\n",
    "id_ventas_validos = set(ventas['id_venta']) if 'id_venta' in ventas.columns else set()\n",
    "\n",
    "# Validar integridad referencial: eliminar huÃ©rfanos\n",
    "if 'id_cliente' in ventas.columns:\n",
    "    ventas = ventas[ventas['id_cliente'].isin(id_clientes_validos)]\n",
    "if 'id_producto' in detalle.columns:\n",
    "    detalle = detalle[detalle['id_producto'].isin(id_productos_validos)]\n",
    "if 'id_venta' in detalle.columns:\n",
    "    detalle = detalle[detalle['id_venta'].isin(id_ventas_validos)]\n",
    "\n",
    "print(f\"âœ“ Ventas con id_cliente vÃ¡lido: {len(ventas)}\")\n",
    "print(f\"âœ“ Detalle con id_producto vÃ¡lido: {len(detalle)}\")\n",
    "print(f\"âœ“ Detalle con id_venta vÃ¡lido: {len(detalle)}\")\n",
    "\n",
    "# Validar sin nulos crÃ­ticos\n",
    "assert clientes['id_cliente'].isnull().sum() == 0, \"âŒ id_cliente tiene nulos\"\n",
    "assert productos['id_producto'].isnull().sum() == 0, \"âŒ id_producto tiene nulos\"\n",
    "assert ventas['id_venta'].isnull().sum() == 0, \"âŒ id_venta tiene nulos\"\n",
    "assert detalle['id_venta'].isnull().sum() == 0, \"âŒ detalle.id_venta tiene nulos\"\n",
    "print(\"âœ“ Sin nulos en claves primarias\")\n",
    "\n",
    "# Validar sin duplicados en PKs\n",
    "assert clientes['id_cliente'].duplicated().sum() == 0, \"âŒ id_cliente duplicado\"\n",
    "assert productos['id_producto'].duplicated().sum() == 0, \"âŒ id_producto duplicado\"\n",
    "assert ventas['id_venta'].duplicated().sum() == 0, \"âŒ id_venta duplicado\"\n",
    "print(\"âœ“ Sin duplicados en claves primarias\")\n",
    "\n",
    "print(\"\\nâœ… Validaciones finales pasadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6019cf",
   "metadata": {},
   "source": [
    "## 9. Conclusiones y ExportaciÃ³n\n",
    "Generamos un resumen del proceso de limpieza, exportamos los datasets limpios y documentamos mÃ©tricas clave.\n",
    "\n",
    "â¬†ï¸ Volver al Ã­ndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "999d8bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ ExportaciÃ³n de datasets limpios:\n",
      "\n",
      "âœ“ clientes_clean.csv: 1,187 filas\n",
      "âœ“ productos_clean.csv: 139 filas\n",
      "âœ“ ventas_clean.csv: 11,728 filas\n",
      "âœ“ detalle_clean.csv: 11,533 filas\n",
      "\n",
      "ðŸ“Š Resumen del proceso de limpieza:\n",
      "\n",
      "         Tabla  Original  Limpio  Filas_eliminadas  %_Retenido\n",
      "      clientes      1296    1187               109       91.59\n",
      "     productos       151     139                12       92.05\n",
      "        ventas     12960   11728              1232       90.49\n",
      "detalle_ventas     12960   11533              1427       88.99\n",
      "\n",
      "âœ“ Resumen exportado: c:\\Users\\LENOVO\\Desktop\\FUNDAMENTOS DE IA GUAYERD\\Diego VÃ¡squez - Proyecto Aurelion\\export\\resumen_limpieza.csv\n",
      "\n",
      "âœ… Proceso de limpieza completo. Datasets listos para EDA y modelado.\n",
      "âœ“ clientes_clean.csv: 1,187 filas\n",
      "âœ“ productos_clean.csv: 139 filas\n",
      "âœ“ ventas_clean.csv: 11,728 filas\n",
      "âœ“ detalle_clean.csv: 11,533 filas\n",
      "\n",
      "ðŸ“Š Resumen del proceso de limpieza:\n",
      "\n",
      "         Tabla  Original  Limpio  Filas_eliminadas  %_Retenido\n",
      "      clientes      1296    1187               109       91.59\n",
      "     productos       151     139                12       92.05\n",
      "        ventas     12960   11728              1232       90.49\n",
      "detalle_ventas     12960   11533              1427       88.99\n",
      "\n",
      "âœ“ Resumen exportado: c:\\Users\\LENOVO\\Desktop\\FUNDAMENTOS DE IA GUAYERD\\Diego VÃ¡squez - Proyecto Aurelion\\export\\resumen_limpieza.csv\n",
      "\n",
      "âœ… Proceso de limpieza completo. Datasets listos para EDA y modelado.\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ’¾ ExportaciÃ³n de datasets limpios:\\n\")\n",
    "\n",
    "# Exportar datasets limpios\n",
    "clientes.to_csv(DIR_DATASETS_LIMPIOS / 'clientes_clean.csv', index=False)\n",
    "productos.to_csv(DIR_DATASETS_LIMPIOS / 'productos_clean.csv', index=False)\n",
    "ventas.to_csv(DIR_DATASETS_LIMPIOS / 'ventas_clean.csv', index=False)\n",
    "detalle.to_csv(DIR_DATASETS_LIMPIOS / 'detalle_clean.csv', index=False)\n",
    "\n",
    "print(f\"âœ“ clientes_clean.csv: {len(clientes):,} filas\")\n",
    "print(f\"âœ“ productos_clean.csv: {len(productos):,} filas\")\n",
    "print(f\"âœ“ ventas_clean.csv: {len(ventas):,} filas\")\n",
    "print(f\"âœ“ detalle_clean.csv: {len(detalle):,} filas\")\n",
    "\n",
    "# Resumen de limpieza\n",
    "print(\"\\nðŸ“Š Resumen del proceso de limpieza:\\n\")\n",
    "resumen = pd.DataFrame({\n",
    "    'Tabla': ['clientes', 'productos', 'ventas', 'detalle_ventas'],\n",
    "    'Original': [len(orig_clientes), len(orig_productos), len(orig_ventas), len(orig_detalle)],\n",
    "    'Limpio': [len(clientes), len(productos), len(ventas), len(detalle)],\n",
    "    'Filas_eliminadas': [\n",
    "        len(orig_clientes) - len(clientes),\n",
    "        len(orig_productos) - len(productos),\n",
    "        len(orig_ventas) - len(ventas),\n",
    "        len(orig_detalle) - len(detalle)\n",
    "    ]\n",
    "})\n",
    "resumen['%_Retenido'] = (resumen['Limpio'] / resumen['Original'] * 100).round(2)\n",
    "print(resumen.to_string(index=False))\n",
    "\n",
    "# Exportar resumen\n",
    "resumen.to_csv(DIR_EXPORT / 'resumen_limpieza.csv', index=False)\n",
    "print(f\"\\nâœ“ Resumen exportado: {DIR_EXPORT / 'resumen_limpieza.csv'}\")\n",
    "\n",
    "print(\"\\nâœ… Proceso de limpieza completo. Datasets listos para EDA y modelado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4be910",
   "metadata": {},
   "source": [
    "### Regla adicional: ImputaciÃ³n de valores faltantes en `ciudad`\n",
    "Esta celda aplica una regla de negocio: cuando la columna `ciudad` estÃ© vacÃ­a o nula en `clientes_clean.csv`, se imputarÃ¡ con \"Buenos Aires\" (in-place). La escritura se realiza sobre `datasets_limpios/clientes_clean.csv` y se deja un listado de casos imputados en `export/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd02b5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imputados': 6, 'faltantes_ciudad_post': 0, 'path': 'c:\\\\Users\\\\LENOVO\\\\Desktop\\\\FUNDAMENTOS DE IA GUAYERD\\\\Diego VÃ¡squez - Proyecto Aurelion\\\\datasets_limpios\\\\clientes_clean.csv'}\n",
      "IDs imputados: [359, 575, 584, 666, 1013, 1067]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Rutas\n",
    "try:\n",
    "    base_export = Path(DIR_EXPORT)\n",
    "except NameError:\n",
    "    base_export = Path('export')\n",
    "base_export.mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    path_clientes = Path(DIR_DATASETS_LIMPIOS) / 'clientes_clean.csv'\n",
    "except NameError:\n",
    "    path_clientes = Path('datasets_limpios') / 'clientes_clean.csv'\n",
    "\n",
    "# Cargar desde disco para robustez\n",
    "_dfc = pd.read_csv(path_clientes)\n",
    "\n",
    "# Normalizar y aplicar imputaciÃ³n\n",
    "_mask = _dfc['ciudad'].replace('', pd.NA).isna()\n",
    "_ids = _dfc.loc[_mask, 'id_cliente'].tolist()\n",
    "_dfc.loc[_mask, 'ciudad'] = 'Buenos Aires'\n",
    "\n",
    "# Guardar IN-PLACE\n",
    "_dfc.to_csv(path_clientes, index=False, encoding='utf-8')\n",
    "\n",
    "# Actualizar variable en sesiÃ³n si existe `clientes`\n",
    "try:\n",
    "    clientes.update(_dfc.set_index('id_cliente'))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ValidaciÃ³n y export de casos\n",
    "_faltantes_post = int(_dfc['ciudad'].replace('', pd.NA).isna().sum())\n",
    "if _ids:\n",
    "    (_dfc[_dfc['id_cliente'].isin(_ids)][['id_cliente','nombre_cliente','email','ciudad','fecha_alta']]\n",
    "        .to_csv(base_export / 'clientes_ciudad_imputada_bsas.csv', index=False, encoding='utf-8'))\n",
    "\n",
    "print({'imputados': len(_ids), 'faltantes_ciudad_post': _faltantes_post, 'path': str(path_clientes)})\n",
    "if _ids:\n",
    "    print('IDs imputados:', _ids)\n",
    "else:\n",
    "    print('No habÃ­a filas con ciudad faltante.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc2c99",
   "metadata": {},
   "source": [
    "### Regla adicional: eliminar productos con nombre vacÃ­o\n",
    "Eliminamos filas de `productos_clean.csv` donde `nombre_producto` estÃ© vacÃ­o o nulo. Se registran las filas removidas en `export/productos_nombre_vacio_eliminados.csv` para trazabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b2a9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se eliminaron 1 filas con nombre vacÃ­o. Archivo de respaldo: c:\\Users\\LENOVO\\Desktop\\FUNDAMENTOS DE IA GUAYERD\\Diego VÃ¡squez - Proyecto Aurelion\\export\\productos_nombre_vacio_eliminados.csv\n",
      "Registro productos limpio guardado: c:\\Users\\LENOVO\\Desktop\\FUNDAMENTOS DE IA GUAYERD\\Diego VÃ¡squez - Proyecto Aurelion\\datasets_limpios\\productos_clean.csv\n",
      "Filas antes: 139 | despuÃ©s: 138\n",
      "ValidaciÃ³n OK: no quedan nombres vacÃ­os.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DIR_DATASETS_LIMPIOS = Path.cwd() / 'datasets_limpios'\n",
    "DIR_EXPORT = Path.cwd() / 'export'\n",
    "DIR_EXPORT.mkdir(exist_ok=True)\n",
    "\n",
    "path_productos = DIR_DATASETS_LIMPIOS / 'productos_clean.csv'\n",
    "df_prod = pd.read_csv(path_productos)\n",
    "antes = len(df_prod)\n",
    "\n",
    "mask_vacio = df_prod['nombre_producto'].isna() | (df_prod['nombre_producto'].astype(str).str.strip() == '')\n",
    "eliminados_df = df_prod.loc[mask_vacio].copy()\n",
    "df_prod = df_prod.loc[~mask_vacio].copy()\n",
    "despues = len(df_prod)\n",
    "\n",
    "if not eliminados_df.empty:\n",
    "    out_removed = DIR_EXPORT / 'productos_nombre_vacio_eliminados.csv'\n",
    "    eliminados_df.to_csv(out_removed, index=False, encoding='utf-8')\n",
    "    print(f'Se eliminaron {len(eliminados_df)} filas con nombre vacÃ­o. Archivo de respaldo: {out_removed}')\n",
    "else:\n",
    "    print('No se encontraron filas con nombre vacÃ­o en productos.')\n",
    "\n",
    "df_prod.to_csv(path_productos, index=False, encoding='utf-8')\n",
    "print(f'Registro productos limpio guardado: {path_productos}')\n",
    "print(f'Filas antes: {antes} | despuÃ©s: {despues}')\n",
    "\n",
    "# ValidaciÃ³n final\n",
    "assert not (df_prod['nombre_producto'].astype(str).str.strip() == '').any(), 'Persisten nombres vacÃ­os tras la limpieza'\n",
    "print('ValidaciÃ³n OK: no quedan nombres vacÃ­os.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e354c5",
   "metadata": {},
   "source": [
    "### Regla adicional: propagar CSAT, CES y NPS a `ventas_clean`\n",
    "Asegura que las columnas `csat_1a5`, `ces_1a5`, `nps_0a10` y `nps_segmento` queden en el export limpio (`CSV/Excel`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df00d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportado ventas limpio con CX -> c:/Users/LENOVO/Desktop/FUNDAMENTOS DE IA GUAYERD/Diego VÃ¡squez - Proyecto Aurelion/datasets_limpios/ventas_clean.csv\n",
      "Columnas: ['id_venta', 'fecha', 'id_cliente', 'medio_pago', 'canal', 'csat_1a5', 'ces_1a5', 'nps_0a10', 'nps_segmento']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Columnas nuevas esperadas\n",
    "cols_cx = ['csat_1a5','ces_1a5','nps_0a10','nps_segmento']\n",
    "for c in cols_cx:\n",
    "    if c not in ventas.columns:\n",
    "        ventas[c] = pd.NA\n",
    "\n",
    "ventas['csat_1a5'] = pd.to_numeric(ventas['csat_1a5'], errors='coerce')\n",
    "ventas['ces_1a5'] = pd.to_numeric(ventas['ces_1a5'], errors='coerce')\n",
    "ventas['nps_0a10'] = pd.to_numeric(ventas['nps_0a10'], errors='coerce')\n",
    "ventas['nps_segmento'] = ventas['nps_segmento'].astype('string')\n",
    "\n",
    "DIR_DATASETS_LIMPIOS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Exportar CSV y Excel preservando columnas\n",
    "csv_out = DIR_DATASETS_LIMPIOS / 'ventas_clean.csv'\n",
    "xlsx_out = DIR_DATASETS_LIMPIOS / 'ventas_clean.xlsx'\n",
    "ventas.to_csv(csv_out, index=False)\n",
    "with pd.ExcelWriter(xlsx_out, engine='xlsxwriter') as writer:\n",
    "    ventas.to_excel(writer, index=False, sheet_name='ventas')\n",
    "\n",
    "print('Exportado ventas limpio con CX ->', csv_out.as_posix())\n",
    "print('Columnas:', list(ventas.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4e3941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportado ventas_clean con CX â†’ c:\\Users\\LENOVO\\Desktop\\FUNDAMENTOS DE IA GUAYERD\\Diego VÃ¡squez - Proyecto Aurelion\\datasets_limpios\\ventas_clean.csv\n",
      "No-nulos CX:\n",
      "csat_1a5        10365\n",
      "ces_1a5          9607\n",
      "nps_0a10         2179\n",
      "nps_segmento     2179\n"
     ]
    }
   ],
   "source": [
    "# -- Persistencia de mÃ©tricas CX en ventas_clean --\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "cols_cx = [\"csat_1a5\", \"ces_1a5\", \"nps_0a10\", \"nps_segmento\"]\n",
    "\n",
    "v_path = DIR_DATASETS / \"ventas.xlsx\"\n",
    "vo = ventas.copy()\n",
    "\n",
    "# Alinear tipos de fecha\n",
    "if \"fecha\" in vo.columns:\n",
    "    vo[\"fecha\"] = pd.to_datetime(vo[\"fecha\"], errors=\"coerce\")\n",
    "\n",
    "if v_path.exists():\n",
    "    v_raw = pd.read_excel(v_path)\n",
    "    v_raw.columns = [str(c).strip() for c in v_raw.columns]\n",
    "    if \"fecha\" in v_raw.columns:\n",
    "        v_raw[\"fecha\"] = pd.to_datetime(v_raw[\"fecha\"], errors=\"coerce\")\n",
    "\n",
    "    # Merge preferente por id_venta, fallback por (id_cliente, fecha)\n",
    "    if \"id_venta\" in vo.columns and \"id_venta\" in v_raw.columns:\n",
    "        vo = vo.merge(\n",
    "            v_raw[[\"id_venta\"] + [c for c in cols_cx if c in v_raw.columns]],\n",
    "            on=\"id_venta\", how=\"left\", suffixes=(\"\", \"_src\")\n",
    "        )\n",
    "    else:\n",
    "        merge_keys = [k for k in [\"id_cliente\", \"fecha\"] if k in vo.columns and k in v_raw.columns]\n",
    "        if merge_keys:\n",
    "            vo = vo.merge(\n",
    "                v_raw[merge_keys + [c for c in cols_cx if c in v_raw.columns]],\n",
    "                on=merge_keys, how=\"left\", suffixes=(\"\", \"_src\")\n",
    "            )\n",
    "\n",
    "    # Completar valores CX desde sufijos _src si aplica\n",
    "    for c in cols_cx:\n",
    "        if c in vo.columns and f\"{c}_src\" in vo.columns:\n",
    "            vo[c] = vo[c].fillna(vo[f\"{c}_src\"])\n",
    "            vo.drop(columns=[f\"{c}_src\"], inplace=True)\n",
    "\n",
    "# Normalizar rangos\n",
    "for c in [\"csat_1a5\", \"ces_1a5\"]:\n",
    "    if c in vo.columns:\n",
    "        vo[c] = pd.to_numeric(vo[c], errors=\"coerce\")\n",
    "        vo.loc[~vo[c].between(1,5), c] = np.nan\n",
    "\n",
    "if \"nps_0a10\" in vo.columns:\n",
    "    vo[\"nps_0a10\"] = pd.to_numeric(vo[\"nps_0a10\"], errors=\"coerce\")\n",
    "    vo.loc[~vo[\"nps_0a10\"].between(0,10), \"nps_0a10\"] = np.nan\n",
    "\n",
    "# Exportar sobrescribiendo ventas_clean\n",
    "out_csv = DIR_DATASETS_LIMPIOS / \"ventas_clean.csv\"\n",
    "out_xlsx = DIR_DATASETS_LIMPIOS / \"ventas_clean.xlsx\"\n",
    "vo.to_csv(out_csv, index=False)\n",
    "with pd.ExcelWriter(out_xlsx, engine=\"xlsxwriter\") as writer:\n",
    "    vo.to_excel(writer, index=False, sheet_name=\"ventas\")\n",
    "\n",
    "# Resumen rÃ¡pido\n",
    "cnt_cx = vo[[c for c in cols_cx if c in vo.columns]].notna().sum()\n",
    "print(\"Exportado ventas_clean con CX â†’\", out_csv)\n",
    "print(\"No-nulos CX:\")\n",
    "print(cnt_cx.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
